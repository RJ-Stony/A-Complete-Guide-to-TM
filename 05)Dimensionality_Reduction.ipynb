{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyObfVrk1NW9i6Vq9MQKxpxH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RJ-Stony/A-Complete-Guide-to-TM/blob/main/05)Dimensionality_Reduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension Reduction using PCA"
      ],
      "metadata": {
        "id": "eo05Us3q1evC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Create a list of topics you want to select from among 20 topics\n",
        "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
        "\n",
        "# Load train dataset\n",
        "# Delete the hint part from the mail content - purely classified by content\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                      remove=('headers', 'footers', 'quotes'),\n",
        "                                      categories=categories)\n",
        "\n",
        "# Load test dataset\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     remove=('headers', 'footers', 'quotes'),\n",
        "                                     categories=categories)"
      ],
      "metadata": {
        "id": "Hxlrqw9a7hUc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S67dSDe9beo",
        "outputId": "b087476f-29f8-46b8-cb6d-727c5722f763"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "X_train = newsgroups_train.data     # Train dataset Document\n",
        "y_train = newsgroups_train.target   # Train dataset Label\n",
        "\n",
        "X_test = newsgroups_test.data       # Test dataset Document\n",
        "y_test = newsgroups_test.target     # Test dataset Label\n",
        "\n",
        "RegTok = RegexpTokenizer(\"[\\w']{3,}\")     # Define tokenizer with Regular expression\n",
        "english_stops = set(stopwords.words('english'))     # Take english stopwords\n",
        "\n",
        "def tokenizer(text):\n",
        "  tokens = RegTok.tokenize(text.lower())\n",
        "  # Except stopwords\n",
        "  words = [word for word in tokens if (word not in english_stops) and len(word) > 2]\n",
        "  # Apply porter stemmer\n",
        "  features = (list(map(lambda token: PorterStemmer().stem(token), words)))\n",
        "  return features\n",
        "\n",
        "tfidf = TfidfVectorizer(tokenizer=tokenizer)\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)      # Transform the train set\n",
        "X_test_tfidf = tfidf.transform(X_test)            # Transform the test set"
      ],
      "metadata": {
        "id": "D-LK0afW9Akg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR_clf = LogisticRegression()     # Classifier Declaration\n",
        "LR_clf.fit(X_train_tfidf, y_train)      # Train a classifier using train data\n",
        "print('# Train set score: {:.3f}'.format(LR_clf.score(X_train_tfidf, y_train)))\n",
        "print('# Test set score: {:.3f}'.format(LR_clf.score(X_test_tfidf, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzQ5KtbO9fl4",
        "outputId": "4191f574-7994-40f5-d468-99d06cda9b70"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train set score: 0.962\n",
            "# Test set score: 0.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2000, random_state=7)\n",
        "X_train_pca = pca.fit_transform(X_train_tfidf.toarray())\n",
        "X_test_pca = pca.transform(X_test_tfidf.toarray())\n",
        "\n",
        "print('Original tfidf matrix shape:', X_train_tfidf.shape)\n",
        "print('PCA Converted matrix shape:', X_train_pca.shape)\n",
        "print(\n",
        "    \"Sum of explained variance ratio: {:.3f}\".format(\n",
        "        pca.explained_variance_ratio_.sum()\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B68gntU_Skh",
        "outputId": "7bc715da-a347-43b5-8584-ac326f665ab7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tfidf matrix shape: (2034, 20085)\n",
            "PCA Converted matrix shape: (2034, 2000)\n",
            "Sum of explained variance ratio: 1.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR_clf.fit(X_train_pca, y_train)\n",
        "print('# Train set score: {:.3f}'.format(LR_clf.score(X_train_pca, y_train)))\n",
        "print('# Test set score: {:.3f}'.format(LR_clf.score(X_test_pca, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOJEZs5gAhjA",
        "outputId": "6d83fb10-e31b-44fc-e2d6-4558f129c875"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train set score: 0.962\n",
            "# Test set score: 0.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear', C=1)\n",
        "lasso_clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print('# Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
        "print('# Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
        "\n",
        "import numpy as np\n",
        "# Output the number of non-zero coefficients\n",
        "print(\n",
        "    \"# Used features count: {}\".format(np.sum(lasso_clf.coef_ != 0)),\n",
        "    \"out of\",\n",
        "    X_train_tfidf.shape[1],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j82dfuS4A2Ng",
        "outputId": "7eb987dc-e351-4b33-ec64-b388275657ac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Train set score: 0.790\n",
            "# Test set score: 0.718\n",
            "# Used features count: 321 out of 20085\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=321, random_state=7)\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train_tfidf.toarray())\n",
        "X_test_pca = pca.transform(X_test_tfidf.toarray())\n",
        "print('PCA Converted X shape:', X_train_pca.shape)\n",
        "print(\n",
        "    \"Sum of explained variance ratio: {:.3f}\".format(\n",
        "        pca.explained_variance_ratio_.sum()\n",
        "    )\n",
        ")\n",
        "\n",
        "LR_clf.fit(X_train_pca, y_train)\n",
        "print('# Train set score: {:.3f}'.format(LR_clf.score(X_train_pca, y_train)))\n",
        "print('# Test set score: {:.3f}'.format(LR_clf.score(X_test_pca, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wa062ch0BeJI",
        "outputId": "c8434a64-e1e0-4e85-aac6-6526e06beeb5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA Converted X shape: (2034, 321)\n",
            "Sum of explained variance ratio: 0.437\n",
            "# Train set score: 0.875\n",
            "# Test set score: 0.751\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=100, random_state=7)\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train_tfidf.toarray())\n",
        "X_test_pca = pca.transform(X_test_tfidf.toarray())\n",
        "print('PCA Converted X shape:', X_train_pca.shape)\n",
        "print(\n",
        "    \"Sum of explained variance ratio: {:.3f}\".format(\n",
        "        pca.explained_variance_ratio_.sum()\n",
        "    )\n",
        ")\n",
        "\n",
        "LR_clf.fit(X_train_pca, y_train)\n",
        "print('# Train set score: {:.3f}'.format(LR_clf.score(X_train_pca, y_train)))\n",
        "print('# Test set score: {:.3f}'.format(LR_clf.score(X_test_pca, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2UEOm7aCDlx",
        "outputId": "a7ec923c-6b4c-43b6-f56e-5aa6b26423bf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PCA Converted X shape: (2034, 100)\n",
            "Sum of explained variance ratio: 0.211\n",
            "# Train set score: 0.807\n",
            "# Test set score: 0.738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension Reduction and Semantics using LSA"
      ],
      "metadata": {
        "id": "XYa73DxOCNDo"
      }
    }
  ]
}